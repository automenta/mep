# MEP Development Roadmap

## Executive Summary

**Status:** EP achieves performance parity with backpropagation on classification tasks (~91-95% MNIST). Core functionality validated with 156 passing tests.

**Mission:** Enable biologically plausible learning research with a performant, well-tested EP implementation.

**Key Achievement:** After systematic bug fixes and parameter optimization, EP now matches Adam/SGD performance on standard classification benchmarks.

**Next Focus:** Technical excellence before outreach‚Äîdemonstrate deep scaling, implement continual learning, optimize speed.

---

## ‚úÖ What's Complete (Foundation Established)

| Component | Status | Notes |
|-----------|--------|-------|
| Core EP Implementation | ‚úÖ | Fully functional, well-tested |
| Performance Parity | ‚úÖ | EP ~91-95% MNIST (matches Adam/SGD) |
| Test Coverage | ‚úÖ | 156 tests passing, 85% coverage |
| Performance Regression Tests | ‚úÖ | Automated baseline monitoring |
| Dropout Compatibility | ‚úÖ | Fixed - dropout skipped during settling |
| Documentation | ‚úÖ | Comprehensive guides and baselines |
| Benchmark Suite | ‚úÖ | MNIST, CIFAR, continual learning |
| CUDA Kernels | ‚úÖ | Fused settling kernel |
| AMP Support | ‚úÖ | Mixed precision compatible |
| torch.compile | ‚úÖ | Compilation compatible |
| Analytic Gradients | ‚úÖ | 1.8x speedup for settling |

### Validated Performance (2026-02-18)

| Benchmark | EP | SGD | Adam | Status |
|-----------|-----|-----|------|--------|
| MNIST (3 epoch) | 91.4% | 91.0% | 90.2% | ‚úÖ EP WINS |
| MNIST (10 epoch) | 95.37% | 93.80% | 95.75% | ‚úÖ EP TIES |
| XOR (100 step) | 100% | 100% | 100% | ‚úÖ PARITY |

---

## üéØ Strategic Research Trajectory

### Phase 1: Solidify Foundation (Q1 2026) - ‚úÖ COMPLETE

**Goal:** Ensure EP performance is stable, documented, and reproducible.

#### Completed
- [x] Fix gradient accumulation bug
- [x] Fix baseline configuration bugs
- [x] Fix dropout incompatibility
- [x] Discover optimal settling parameters
- [x] Achieve performance parity with backprop
- [x] Create performance regression tests
- [x] Document performance baselines

**Success Criteria:** ‚úÖ All met

---

### Phase 2: Technical Excellence (Q2-Q3 2026) - HIGH PRIORITY

**Goal:** Achieve compelling technical advantages before external outreach.

**Philosophy:** Build undeniable results first, then share. Partnerships are more productive when we have clear advantages to demonstrate.

**Detailed Plans:**
- [phase2_detailed_plan.md](phase2_detailed_plan.md) ‚Äî Full 6-month plan
- [phase2_week1-2.md](phase2_week1-2.md) ‚Äî Immediate action items

---

#### Priority 1: O(1) Memory Implementation ‚úÖ COMPLETE (Revised Understanding)

**Original Hypothesis:** EP can achieve O(1) activation memory by avoiding unnecessary PyTorch functionality.

**Finding (Week 1-4, 2026-02-25):** EP inherently requires O(depth) memory for:
- State storage (free phase + nudged phase states)
- Parameter gradient computation (contrast step)

**Revised Understanding:** EP achieves **O(1) settling overhead** - the settling loop doesn't accumulate additional memory beyond O(depth) state storage. This is already achieved by the current implementation.

**Key Deliverables:**
- ‚úÖ Memory profiling established baseline (0.1326 MB/layer)
- ‚úÖ Component breakdown: Settling 32%, Energy 32%, Contrast 36%
- ‚úÖ Analytic gradients implemented (1.8x speedup)
- ‚úÖ Correctness verified (2.5e-12 gradient match)

**Why this matters:**
- Clarifies EP's actual memory characteristics
- 1.8x speedup from analytic gradients is valuable
- Enables focus on genuine EP advantages

**Technical Approach (Completed):**

1. ‚úÖ **Avoid PyTorch Autograd Overhead** - Analytic gradients avoid autograd during settling
2. ‚úÖ **Minimize Intermediate Activations** - States updated in-place
3. ‚úÖ **Gradient Checkpointing for EP** - Implemented for contrast step
4. ‚úÖ **Custom CUDA Kernels** - Fused settling kernel exists

**Action Items (Complete):**
- [x] Profile current memory usage by component
- [x] Identify PyTorch operations triggering activation storage
- [x] Implement manual settling without autograd
- [x] Implement analytic state gradients
- [x] Test at extreme depths (100, 500, 1000 layers)
- [x] Document findings (O(1) settling overhead, not O(1) total)

**Success Criteria:**
- ‚úÖ EP settling overhead is O(1) (not O(steps √ó depth))
- ‚úÖ Analytic gradients match autograd (< 1e-5 difference)
- ‚úÖ Training produces identical results (< 1e-3 loss difference)
- ‚úÖ Speed improvement demonstrated (1.8x achieved)

**Timeline:** ‚úÖ Complete (Week 1-4, 2026-02-25)
**Impact:** Medium - clarifies memory characteristics, provides 1.8x speedup

---

#### Priority 2: Deep Network Scaling üî¥ CURRENT FOCUS

**Hypothesis:** EP can train networks at depths (5000-10000+ layers) that are impractical for backpropagation due to memory constraints.

**Why this matters:**
- Demonstrates EP's practical advantage: stable training at extreme depths
- Even with O(depth) memory, EP may scale better than backprop due to:
  - No vanishing/exploding gradients (local settling dynamics)
  - Muon orthogonalization maintains gradient flow
  - Error feedback recovers information

**Action Items:**
- [ ] Create deep scaling test script
- [ ] Test at 1000, 2000, 5000, 10000 layer depths
- [ ] Measure: memory, convergence, accuracy, gradient norms
- [ ] Compare vs backprop at each depth
- [ ] Document scaling behavior and any failure modes

**Success Criteria:**
- EP trains stably at 5000+ layers
- EP achieves reasonable accuracy (>70% MNIST) at 1000+ layers
- Clear scaling curves documented
- Failure modes (if any) identified and characterized

**Timeline:** 2-3 weeks (Week 5-7)
**Impact:** High - demonstrates EP's unique capability

---

#### Priority 3: Continual Learning (Technical Foundation)

**Hypothesis:** EP + proper CL methods can reduce catastrophic forgetting.

**Current Status:** Error feedback reduces forgetting (32% vs 48%) but EWC is more effective (5-15%).

**Action Items:**
- [ ] Implement EWC integration for EP
- [ ] Test on standard CL benchmarks (Permuted MNIST, Split CIFAR)
- [ ] Compare EP+EWC vs backprop+EWC
- [ ] Analyze why EP+EF reduces forgetting
- [ ] Publish technical report with results

**Timeline:** 2-3 months
**Impact:** Medium-High - CL is important research area

---

#### Priority 4: Speed Optimization

**Hypothesis:** Settling overhead can be reduced without losing convergence.

**Current Status:** EP is 2-3√ó slower than backprop (fundamental settling cost).

**Action Items:**
- [ ] Profile settling time by component
- [ ] Test adaptive settling (early stopping)
- [ ] Optimize CUDA kernels
- [ ] Explore approximate settling methods
- [ ] Document speed/accuracy tradeoffs

**Timeline:** 1-2 months
**Impact:** Medium - speed is important for adoption

---

### Phase 3: Results & Outreach (Q4 2026+) - CONTINGENT

**Goal:** Share compelling results with research community.

**Prerequisites:**
- ‚úÖ O(1) settling overhead demonstrated (Week 1-4)
- ‚úÖ 1.8x speedup from analytic gradients (Week 1-4)
- ‚è≥ Deep scaling results (5000+ layers) - In Progress
- ‚è≥ CL results (EP+EWC competitive) - Planned
- ‚è≥ Speed optimizations complete - In Progress

**Only after Phase 2 is complete:**

#### Neuromorphic Partnerships
- [ ] Reach out to Intel Labs (Loihi)
- [ ] Reach out to SpiNNaker group
- [ ] Benchmark on neuromorphic hardware
- [ ] Publish energy efficiency study

#### Biological Plausibility Research
- [ ] Partner with computational neuroscience labs
- [ ] Compare EP dynamics to neural data
- [ ] Publish biological plausibility study

#### Community Building
- [ ] Release comprehensive benchmark suite
- [ ] Write tutorial/guide papers
- [ ] Present at relevant venues (NeurIPS, ICLR, CNS)

---

## üìä Success Metrics

| Metric | Current | Target (6mo) | Target (12mo) |
|--------|---------|-------------|---------------|
| MNIST Accuracy | 95.37% | 95%+ (maintain) | 95%+ (maintain) |
| Test Coverage | 85% | 85%+ (maintain) | 90% |
| Memory Scaling | O(depth) | **O(1)** | O(1) verified |
| Max Depth Tested | 2000 | **5000+** | 10000+ |
| CL Forgetting | 32% (EF) | **<15%** (EWC) | <10% |
| Speed vs BP | 2-3√ó slower | **1.5-2√ó** | 1.5√ó |
| External Contributors | 0 | 2+ | 10+ |
| GitHub Stars | ~0 | 50+ | 200+ |
| Citations | 0 | 0 (pre-results) | 10+ (post-results) |

---

## üî¨ Open Research Questions

### Technical Questions (Phase 2 Focus)
1. **Can EP achieve O(1) memory?** What PyTorch features trigger activation storage?
2. **What's the maximum trainable depth?** 5000? 10000? 100000+ layers?
3. **Can settling be accelerated?** Without losing convergence?
4. **Does EP+EWC outperform backprop+EWC?** On standard CL benchmarks?

### Scientific Questions (Phase 3 Focus)
1. **What is EP's true niche?** Where does it genuinely excel vs backprop?
2. **Can EP train transformers?** What architectural changes are needed?
3. **Is EP more energy-efficient?** Under what conditions?
4. **Does biological plausibility matter?** For what applications?

---

## üöß Known Limitations (Honest Assessment)

| Limitation | Status | Plan |
|------------|--------|------|
| Memory usage | üî¥ **Priority** | O(1) implementation in progress |
| Training speed | ‚ö†Ô∏è Acceptable | 2√ó slowdown for biological plausibility is reasonable |
| Dropout incompatibility | ‚úÖ Fixed | Skip dropout during settling |
| Continual learning | üî¥ In progress | EWC integration planned |
| Very deep networks | üî¥ Untested | Waiting for O(1) memory |

---

## üìÅ Key Documentation

| Document | Purpose |
|----------|---------|
| [README.md](../README.md) | Quick start, how MEP works |
| [docs/index.md](../docs/index.md) | Documentation hub |
| [docs/benchmarks/PERFORMANCE_BASELINES.md](../docs/benchmarks/PERFORMANCE_BASELINES.md) | Performance thresholds, optimal config |
| [docs/benchmarks/VALIDATION_RESULTS.md](../docs/benchmarks/VALIDATION_RESULTS.md) | Full validation study |
| [docs/methods_paper.md](../docs/methods_paper.md) | Preprint-ready methods paper |

---

## üí° Why This Roadmap

**Phase 2 before Phase 3:** Outreach is more effective with compelling results. A partnership proposal that says "EP matches backprop accuracy" is good. One that says "EP matches backprop AND achieves O(1) memory AND trains 10000-layer networks" is irresistible.

**Technical excellence first:**
1. Solve the memory problem (O(1) implementation)
2. Demonstrate unique capabilities (deep scaling)
3. Show competitive CL performance (EP+EWC)
4. Then share with the world

**This approach:**
- Avoids premature hype
- Builds genuine advantages
- Makes partnerships more productive
- Establishes MEP as serious research tool

---

## üìÖ Immediate Action Items (Next 2 Weeks)

- [ ] Profile memory usage by component
- [ ] Identify PyTorch operations triggering activation storage
- [ ] Design O(1) memory settling implementation
- [ ] Set up deep network scaling test infrastructure
- [ ] Review EWC implementation for EP integration

---

## üéì Final Thought

**We've proven EP can match backprop on accuracy.** That was step one.

**Step two is proving EP's theoretical advantages in practice:** O(1) memory, deep scaling, continual learning.

**Step three is sharing those results** with researchers who can benefit from them.

This roadmap prioritizes substance over hype, results over partnerships, and technical excellence over premature announcements.

**Success =** MEP becomes the standard tool for EP research, with demonstrated advantages that speak for themselves.

---

*Last updated: 2026-02-18*
*Status: Foundation complete, technical excellence phase beginning*
