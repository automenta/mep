# MEP Development Roadmap

## Strategic Priorities for Maximizing Impact

**Status:** TODO.md complete. Core functionality implemented and tested. Focus now shifts to **demonstrating unique value**.

---

## âœ… What's Done (TODO.md Complete)

| Item | Status | Notes |
|------|--------|-------|
| Comprehensive tests | âœ… | 142 unit tests, 85% coverage |
| Numerical gradient verification | âœ… | EP vs BP comparison tests |
| Type hints & mypy | âœ… | `mypy mep/optimizers/` passes |
| Input validation & NaN guards | âœ… | Robust error handling |
| Adaptive settling | âœ… | Early stopping implemented |
| Convolutional layers | âœ… | Conv2d, BatchNorm2d support |
| Mixed-precision (AMP) | âœ… | Implemented and tested |
| torch.compile | âœ… | Compatible and tested |
| NaturalGradient Fisher | âœ… | Working implementation |
| Transformer/attention support | âœ… | LayerNorm, residuals, MHA support |
| Dion CUDA kernels | âœ… | Fused settling kernel added |
| Benchmark suite | âœ… | `mep/benchmarks/` complete |
| Working examples | âœ… | `quickstart.py`, `mnist_comparison.py`, `train_char_lm.py` |
| Error feedback fix | âœ… | Works correctly with Dion, disabled for Muon |

**MNIST Result:** ~89% test accuracy with EP (vs ~92% backprop)

**Note:** Transformer architecture support exists, but full transformer-based LM training (e.g., nanoGPT-style) remains to be validated as part of the Character LM goal.

---

## ğŸ¯ Immediate Priorities (Week 1-2)

### 1. Prove Memory Advantage [HIGHEST PRIORITY]

**Why:** EP's O(1) memory claim is theoretical. This is EP's clearest quantitative advantage over backprop.

**What:**
```python
# examples/memory_comparison.py
# Train networks of increasing depth (10, 50, 100, 200, 500 layers)
# Measure: peak memory, training stability, final accuracy
# Find the depth where backprop OOMs but EP succeeds
```

**Success criteria:**
- [ ] EP trains a network that backprop cannot (CUDA OOM)
- [ ] Memory vs depth plot showing O(1) vs O(depth) scaling
- [ ] Document the crossover point (e.g., "EP uses 50% less memory at 100 layers")

**Impact:** If true, this is a unique, defensible advantage.

**Effort:** 2-3 days

---

### 2. Validate Continual Learning with Working EF

**Why:** This is EP's claimed "killer app" but needs proof with the fixed error feedback.

**What:**
```bash
python -m mep.benchmarks.continual_learning
# Permuted MNIST: 5+ tasks
# Compare: EP+EF vs EP-without-EF vs SGD
# Metric: average forgetting, final accuracy
```

**Success criteria:**
- [ ] Benchmark runs end-to-end without errors
- [ ] EP+EF shows reduced forgetting vs EP-without-EF
- [ ] Results are reproducible across seeds

**Impact:** Validates the continual learning hypothesis.

**Effort:** 3-5 days

---

### 3. Validate Character LM Example

**Why:** Shows EP works beyond classification, on sequential prediction tasks.

**What:**
```bash
python examples/train_char_lm.py
# Train EP and backprop on Shakespeare character LM
# Compare: loss curves, generated text quality, training dynamics
```

**Success criteria:**
- [ ] Script runs without errors
- [ ] Generated text shows learning (coherent characters)
- [ ] Document qualitative differences (EP vs backprop outputs)

**Follow-up:** Extend to transformer-based LM (e.g., nanoGPT-style) to demonstrate EP works with attention architectures.

**Impact:** Demonstrates EP's applicability to language tasks and transformer architectures.

**Effort:** 1-2 days (LSTM), 1-2 weeks (transformer extension)

---

### 4. Write "Why EP?" Guide

**Why:** Users need honest guidance on when EP is worth the complexity.

**What:** Add to README or create standalone doc:
- When to use EP (memory-constrained, continual learning, research)
- When NOT to use EP (standard classification, production, speed-critical)
- EP's unique characteristics vs backprop (already started in README)

**Success criteria:**
- [ ] Clear decision tree for users
- [ ] Honest about tradeoffs
- [ ] Links to relevant examples

**Effort:** 1 day

---

## ğŸš€ Medium-Term Priorities (Month 1-2)

### 5. Deep Network Scaling Study

**Why:** If EP's memory advantage is real, it should enable training deeper networks.

**What:**
- Train ResNet-style networks at depths: 10, 50, 100, 200, 500 layers
- Measure: training stability, final accuracy, memory, time
- Compare EP vs backprop at each depth

**Hypothesis:** EP's advantage grows with depth.

**Success criteria:**
- [ ] EP successfully trains networks where backprop fails or is unstable
- [ ] Document depth limits for both methods

**Effort:** 1-2 weeks (compute time)

---

### 6. Add Proper Baselines to Benchmarks

**Why:** Current benchmarks compare EP to... itself. Need external baselines.

**What:**
- SGD, Adam (standard optimization baselines)
- EWC, GEM (continual learning baselines)
- Muon-only (ablate the EP component)

**Success criteria:**
- [ ] All benchmarks include at least 2 external baselines
- [ ] Results show where EP wins and loses

**Effort:** 1 week

---

### 7. Publish Results (If Warranted)

**Why:** If any of the above shows EP has unique value, share it.

**What:**
- Workshop paper (NeurIPS, ICLR, ICML)
- Blog post with interactive notebooks
- Conference talk

**Success criteria:**
- [ ] Clear contribution (e.g., "EP enables X that backprop cannot")
- [ ] Reproducible experiments
- [ ] Open-source code release

**Effort:** 2-4 weeks

---

## ğŸ—ï¸ Long-Term Priorities (Month 3+)

### 8. Find a Domain Where EP Wins

**Candidates:**
| Domain | Hypothesis | Validation Needed |
|--------|------------|-------------------|
| Continual learning | EP+EF reduces forgetting | Benchmark vs EWC, GEM |
| Memory-constrained | EP trains larger models | Memory scaling study |
| Edge devices | EP fits on-device | Deployment demo |
| Privacy-preserving | No stored activations | Formal analysis |
| Neuromorphic hardware | Natural fit | Partnership required |

**Success criteria:**
- [ ] One domain where EP has clear, reproducible advantage
- [ ] Published results or demo

**Effort:** 2-3 months

---

### 9. PyTorch Lightning Integration [OPTIONAL]

**Status:** Not implemented. Consider if community interest emerges.

**What:**
```python
# Target API (if implemented)
from lightning.pytorch import LightningModule
from mep.integrations import MEPOptimizer

class MyModel(LightningModule):
    def configure_optimizers(self):
        return smep(self.parameters(), model=self, mode='ep')
```

**Rationale:** Only worth the effort if MEP gains adoption and users request it. Not prioritized until core value is proven.

**Effort:** 2-3 days (if/when needed)

---

### 10. Advanced CUDA Optimization [ON HOLD]

**Status:** Fused settling kernel implemented (`fused_settle_step_inplace`). Further optimization deferred.

**What could be added:**
- Custom SVD kernel for Dion
- Mixed-precision settling

**Rationale:** Don't optimize until we know EP has a use case worth optimizing for. Current 1.5Ã— slowdown is acceptable for research.

**Effort:** 4-8 weeks (if/when needed)

---

## ğŸ“Š Effort vs. Impact Matrix

```
Impact
  â–²
  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚    â”‚ 1. Memory       â”‚    â”‚ 8. Domain    â”‚
  â”‚    â”‚    Advantage    â”‚    â”‚    Where EP  â”‚
  â”‚    â”‚ 2. Continual    â”‚    â”‚    Wins      â”‚
  â”‚    â”‚    Learning     â”‚    â”‚ 9. Lightning â”‚
  â”‚    â”‚ 5. Deep Scaling â”‚    â”‚    (optional)â”‚
  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ 10. CUDA     â”‚
  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚     Opt.     â”‚
  â”‚    â”‚ 3. Char LM  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚    â”‚ 4. Why EP?  â”‚
  â”‚    â”‚ 6. Baselinesâ”‚
  â”‚    â”‚ 7. Publish  â”‚
  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Effort
       Low                    High
```

---

## ğŸ¯ Recommended Sequence

### Phase 1: Prove Value (Weeks 1-2)
1. Memory advantage demonstration
2. Continual learning validation
3. Character LM example (includes transformer extension)
4. "Why EP?" documentation

**Goal:** Establish clear, honest value proposition

### Phase 2: Deepen Evidence (Weeks 3-8)
5. Deep network scaling study
6. Add proper baselines
7. Publish results (if warranted)

**Goal:** Build credible evidence for EP's niche

### Phase 3: Expand Impact (Months 3-6)
8. Find domain where EP wins
9. Community building
10. Lightning/CUDA optimization (if value proven and users request)

**Goal:** Establish MEP as the go-to EP framework

---

## ğŸ“ˆ Success Metrics

| Metric | Current | Target (3mo) | Target (12mo) |
|--------|---------|--------------|---------------|
| Working examples | 4 | 6 | 10+ |
| Documented use cases | 1 (classification) | 3 | 5+ |
| External contributors | 0 | 1+ | 5+ |
| GitHub Stars | ~0 | 50+ | 200+ |
| Citations | 0 | 1+ | 10+ |
| EP Speed (vs backprop) | ~1.5Ã— slower | ~1.5Ã— slower | ~1.2Ã— slower |
| Memory advantage | Unproven | Demonstrated | Quantified |
| Continual learning | Unproven | Validated | Competitive |

---

## ğŸ” Research Questions to Answer

1. **Does EP's O(1) memory enable training deeper networks?** â† Immediate focus
2. **Does EP+EF reduce catastrophic forgetting?** â† Immediate focus
3. **What domains benefit from EP's qualitative differences?** â† Medium-term
4. **Is EP's speed penalty acceptable for its advantages?** â† Ongoing
5. **Does biological plausibility matter practically?** â† Long-term

---

## ğŸ¤ Collaboration Opportunities

| Domain | Potential Partners | Priority |
|--------|-------------------|----------|
| Continual Learning | CL research groups | High |
| Memory-Efficient DL | Systems/ML groups | High |
| Energy-Based Models | Yann LeCun's group | Medium |
| Neuromorphic Hardware | Intel Labs, SpiNNaker | Low (until software value proven) |

---

## ğŸ“ Immediate Action Items

### This Week
- [ ] Create `examples/memory_comparison.py`
- [ ] Run and validate `examples/train_char_lm.py`
- [ ] Run continual learning benchmark with fixed EF

### This Month
- [ ] Complete memory advantage demonstration
- [ ] Write "Why EP?" guide
- [ ] Add SGD/Adam baselines to benchmarks

### This Quarter
- [ ] Deep network scaling study
- [ ] Publish results (if value demonstrated)
- [ ] Community outreach (blog, social)

---

## ğŸ’¡ Final Thought

**The goal is not to replace backpropagation.** The goal is to:
1. Enable biologically plausible learning research
2. Find niches where EP excels (memory, continual learning, hardware)
3. Push the boundaries of what's possible with local learning rules

**Success = MEP becomes the go-to framework for EP research, with clear evidence of when and why to use it.**

---

## Appendix: What We've Learned

### What Works
- âœ… EP trains classification models (~89% MNIST)
- âœ… Gradients flow through all layers
- âœ… Adaptive settling reduces overhead (~1.5Ã— vs 3Ã— slowdown)
- âœ… Error feedback works with Dion (not Muon)

### What's Unclear
- â“ Does O(1) memory actually enable deeper networks?
- â“ Does EP+EF reduce forgetting in continual learning?
- â“ Are there tasks where EP outperforms backprop?

### What's Next
The next 2 weeks focus on answering the "What's Unclear" questions with concrete experiments.
