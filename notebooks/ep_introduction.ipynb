{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Equilibrium Propagation (EP)\n",
        "\n",
        "Equilibrium Propagation (EP) is a biologically plausible alternative to backpropagation. Instead of propagating error signals backward through the network (which requires symmetric weights and separate phases), EP uses local learning rules based on the contrast between two phases of network dynamics:\n",
        "\n",
        "1. **Free Phase**: The network settles to a state that minimizes its internal energy, given an input $x$.\n",
        "2. **Nudged Phase**: The output layer is nudged towards the target $y$, and the network settles to a new state.\n",
        "\n",
        "The gradient is estimated as:\n",
        "$$ \\nabla_W L \\approx \\frac{1}{\\beta} (s_i^{\\text{nudged}} s_j^{\\text{nudged}} - s_i^{\\text{free}} s_j^{\\text{free}}) $$\n",
        "\n",
        "This tutorial demonstrates how to use the `mep` library to train a simple neural network using EP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from mep import smep\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "We use the standard MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\"../data\", train=True, download=True, transform=transform),\n",
        "    batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "# Visualize one batch\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "plt.imshow(images[0].reshape(28, 28), cmap=\"gray\")\n",
        "plt.title(f\"Label: {labels[0].item()}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Definition\n",
        "We define a simple Multi-Layer Perceptron (MLP). Note that we use standard `nn.Sequential`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Optimizer Setup (SMEP)\n",
        "We use the `smep` preset, which combines:\n",
        "- **Spectral Normalization**: Ensures stability of the fixed point dynamics.\n",
        "- **Muon Update**: Orthogonalizes gradients for better conditioning.\n",
        "- **Equilibrium Propagation**: Computes gradients without backprop.\n",
        "\n",
        "Crucially, we set `mode=\"ep\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = smep(\n",
        "    model.parameters(),\n",
        "    model=model,        # Pass model for EP to access structure\n",
        "    mode=\"ep\",          # Enable EP\n",
        "    lr=0.05,\n",
        "    beta=0.5,           # Nudging strength\n",
        "    settle_steps=15,    # Steps for settling dynamics\n",
        "    loss_type=\"cross_entropy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Loop\n",
        "The training loop is standard, but `optimizer.step()` handles the forward passes (free and nudged phases) internally. We don t call `loss.backward()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.train()\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # EP Step: Free phase -> Nudged phase -> Update\n",
        "        optimizer.step(x=data, target=target)\n",
        "        \n",
        "        # Calculate accuracy (optional, requires extra forward pass)\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += target.size(0)\n",
        "            \n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1} | Batch {batch_idx}/{len(train_loader)} | Acc: {100. * correct / total:.2f}%\")\n",
        "            correct = 0\n",
        "            total = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "You have successfully trained a neural network using Equilibrium Propagation! This method avoids the biological implausibility of backpropagation while achieving competitive results on simple tasks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}